"""
レース詳細情報スクレイパー（pandas.read_html()方式 - Zenn完全準拠版）
2024年11月のnetkeiba仕様変更に完全対応
- User-Agentランダム化（12種類）
- スクレイピング間隔ランダム化（2-3秒）
- 高速（Seleniumの6-10倍）
"""
import pandas as pd
import requests
import sqlite3
import json
import time
import random
import re
from typing import Dict, List, Optional, Tuple
from datetime import datetime
from pathlib import Path
from loguru import logger


class RaceDetailScraperPandas:
    """pandas.read_html()を使用した高速スクレイパー"""
    
    # Zennの記事で推奨されているUser-Agentリスト（2024年11月対応）
    USER_AGENTS = [
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/115.0.0.0 Safari/537.36",
        "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/115.0.0.0 Safari/537.36",
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:115.0) Gecko/20100101 Firefox/115.0",
        "Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:115.0) Gecko/20100101 Firefox/115.0",
        "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.1.2 Safari/605.1.15",
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/115.0.0.0 Safari/537.36 Edg/115.0.0.0",
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/115.0.0.0 Safari/537.36 OPR/85.0.4341.72",
        "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/115.0.0.0 Safari/537.36 OPR/85.0.4341.72",
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/115.0.0.0 Safari/537.36 Vivaldi/5.3.2679.55",
        "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/115.0.0.0 Safari/537.36 Vivaldi/5.3.2679.55",
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/115.0.0.0 Safari/537.36 Brave/1.40.107",
        "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/115.0.0.0 Safari/537.36 Brave/1.40.107",
    ]
    
    def __init__(self, db_path="data/keiba.db"):
        """初期化"""
        self.db_path = db_path
        
    def scrape_and_update(self, race_id: str, max_retries: int = 3) -> bool:
        """
        レース詳細を取得してJSONとDBの両方に保存
        
        Args:
            race_id: レースID
            max_retries: 最大リトライ回数
            
        Returns:
            成功時True、失敗時False
        """
        for attempt in range(1, max_retries + 1):
            try:
                logger.info(f"Scraping race_id={race_id} (attempt {attempt}/{max_retries})")
                
                # スクレイピング実行
                race_data = self._scrape_race_details(race_id)
                
                if not race_data or not race_data.get('race_info'):
                    logger.warning(f"No data retrieved for race_id={race_id}")
                    if attempt < max_retries:
                        # Zenn推奨：2-3秒のランダム待機
                        wait_time = random.uniform(2.0, 3.0)
                        logger.debug(f"Waiting {wait_time:.1f} seconds before retry...")
                        time.sleep(wait_time)
                        continue
                    return False
                
                # JSONファイルに保存
                self._save_json(race_id, race_data)
                
                # DB更新
                success = self._update_database(race_id, race_data)
                
                if success:
                    logger.info(f"✅ Successfully updated race_id={race_id}")
                    return True
                else:
                    if attempt < max_retries:
                        wait_time = random.uniform(2.0, 3.0)
                        logger.debug(f"Waiting {wait_time:.1f} seconds before retry...")
                        time.sleep(wait_time)
                        continue
                    return False
                    
            except Exception as e:
                logger.error(f"Error in attempt {attempt} for race_id={race_id}: {e}")
                if attempt < max_retries:
                    wait_time = random.uniform(2.0, 3.0)
                    logger.debug(f"Waiting {wait_time:.1f} seconds before retry...")
                    time.sleep(wait_time)
                    continue
                return False
        
        return False
    
    def _scrape_race_details(self, race_id: str) -> Optional[Dict]:
        """レース詳細をpandas.read_html()でスクレイピング"""
        url = f"https://db.netkeiba.com/race/{race_id}"
        
        try:
            # User-Agentをランダムに選択
            selected_user_agent = random.choice(self.USER_AGENTS)
            headers = {
                "User-Agent": selected_user_agent,
                "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8",
                "Accept-Language": "ja-JP,ja;q=0.9,en-US;q=0.8,en;q=0.7",
            }
            
            logger.debug(f"Selected User-Agent: {selected_user_agent[:80]}...")
            
            # HTTPリクエスト（User-Agent付き）
            response = requests.get(url, headers=headers, timeout=30)
            response.raise_for_status()  # HTTPエラーをチェック
            
            # Zenn推奨：2-3秒のランダム待機
            wait_time = random.uniform(2.0, 3.0)
            logger.debug(f"Waiting {wait_time:.1f} seconds after request...")
            time.sleep(wait_time)
            
            # HTMLからテーブルを取得
            tables = pd.read_html(response.content)
            
            # BeautifulSoupでメタ情報を取得
            from bs4 import BeautifulSoup
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # レース情報を抽出
            race_info = self._extract_race_info(soup)
            
            # レース結果を抽出（table[0]）
            race_results = self._extract_race_results(tables[0]) if len(tables) > 0 else []
            
            # 払い戻し情報を抽出（table[1], table[2]）
            payback_info = self._extract_payback_info(tables)
            
            # コーナー通過順を抽出
            corner_pass = self._extract_corner_pass(soup)
            
            # ラップタイムを抽出
            lap_times = self._extract_lap_times(soup)
            
            return {
                'race_id': race_id,
                'race_info': race_info,
                'race_results': race_results,
                'payback': payback_info,
                'corner_pass': corner_pass,
                'lap_times': lap_times,
                'scraped_at': datetime.now().isoformat()
            }
            
        except requests.exceptions.RequestException as e:
            logger.error(f"HTTP request error for race {race_id}: {e}")
            return None
        except Exception as e:
            logger.error(f"Error scraping race {race_id}: {e}")
            return None
    
    def _extract_race_info(self, soup) -> Dict:
        """レース基本情報をBeautifulSoupで抽出"""
        info = {}
        
        try:
            # レース名
            race_name_elem = soup.find('div', class_='RaceName')
            if race_name_elem:
                info['race_name'] = race_name_elem.get_text(strip=True).split('\n')[0]
            
            # グレード
            grade_elem = soup.find('span', class_=lambda x: x and 'Icon_GradeType' in x)
            if grade_elem:
                grade_class = grade_elem.get('class', [])
                class_list = [c for c in grade_class if isinstance(c, str)]
                if 'Icon_GradeType1' in class_list:
                    info['grade'] = 'G1'
                elif 'Icon_GradeType2' in class_list:
                    info['grade'] = 'G2'
                elif 'Icon_GradeType3' in class_list:
                    info['grade'] = 'G3'
                else:
                    info['grade'] = None
            else:
                info['grade'] = None
            
            # RaceData01（発走時刻、距離、馬場状態など）
            race_data01_elem = soup.find('div', class_='RaceData01')
            if race_data01_elem:
                race_data01_text = race_data01_elem.get_text(strip=True)
                
                # 発走時刻
                time_match = re.search(r'(\d{1,2}:\d{2})発走', race_data01_text)
                info['post_time'] = time_match.group(1) if time_match else None
                
                # 芝/ダート、距離
                distance_match = re.search(r'(芝|ダ)\s*(\d+)m', race_data01_text)
                if distance_match:
                    track_type_raw = distance_match.group(1)
                    info['track_type'] = 'ダート' if track_type_raw == 'ダ' else track_type_raw
                    info['distance'] = int(distance_match.group(2))
                else:
                    info['track_type'] = '不明'
                    info['distance'] = 0
                
                # 天候
                weather_match = re.search(r'天候:([^\s/]+)', race_data01_text)
                info['weather'] = weather_match.group(1) if weather_match else None
                
                # 馬場状態
                track_condition_match = re.search(r'馬場:([^\s/]+)', race_data01_text)
                info['track_condition'] = track_condition_match.group(1) if track_condition_match else None
            
            # RaceData02（開催情報、賞金など）
            race_data02_elem = soup.find('div', class_='RaceData02')
            if race_data02_elem:
                race_data02_text = race_data02_elem.get_text(strip=True)
                
                # 開催回、競馬場、日目
                kaisai_match = re.search(r'(\d+)回\s*(\S+?)\s*(\d+)日目', race_data02_text)
                if kaisai_match:
                    info['kaisai_count'] = int(kaisai_match.group(1))
                    info['venue'] = kaisai_match.group(2)
                    info['day'] = int(kaisai_match.group(3))
                else:
                    info['venue'] = '不明'
                
                # 出走頭数
                horses_match = re.search(r'(\d+)頭', race_data02_text)
                info['horse_count'] = int(horses_match.group(1)) if horses_match else 0
                
                # 本賞金（1位のみ）
                prize_match = re.search(r'本賞金:(\d+)', race_data02_text)
                info['prize_money'] = int(prize_match.group(1)) if prize_match else 0
                
                # レースクラス
                class_match = re.search(r'(オープン|1600万|1000万|500万|未勝利|新馬|[１-３]勝クラス)', race_data02_text)
                info['race_class'] = class_match.group(1) if class_match else None
                
                # 馬齢・重量条件
                condition_match = re.search(r'(サラ系|アラブ系)([^\s]+)', race_data02_text)
                info['race_condition'] = condition_match.group(0) if condition_match else None
                
                weight_match = re.search(r'(ハンデ|定量|別定)', race_data02_text)
                info['weight_type'] = weight_match.group(1) if weight_match else None
            
            logger.debug(f"Extracted: venue={info.get('venue')}, track={info.get('track_type')}, distance={info.get('distance')}")
            
        except Exception as e:
            logger.error(f"Error extracting race info: {e}")
        
        return info
    
    def _extract_race_results(self, df: pd.DataFrame) -> List[Dict]:
        """レース結果をDataFrameから抽出"""
        results = []
        
        try:
            # DataFrameの列名を確認してマッピング
            # 通常: 着順、枠、馬番、馬名、性齢、斤量、騎手、タイム、着差、人気、単勝オッズ、後3F、...
            
            for idx, row in df.iterrows():
                try:
                    result = {}
                    
                    # 基本的な列マッピング（列の位置で取得）
                    result['rank'] = int(row.iloc[0]) if pd.notna(row.iloc[0]) else 0
                    result['bracket'] = int(row.iloc[1]) if pd.notna(row.iloc[1]) else 0
                    result['horse_number'] = int(row.iloc[2]) if pd.notna(row.iloc[2]) else 0
                    result['horse_name'] = str(row.iloc[3]) if pd.notna(row.iloc[3]) else ""
                    result['sex_age'] = str(row.iloc[4]) if pd.notna(row.iloc[4]) else ""
                    result['jockey_weight'] = float(row.iloc[5]) if pd.notna(row.iloc[5]) else 0.0
                    result['jockey'] = str(row.iloc[6]) if pd.notna(row.iloc[6]) else ""
                    result['time'] = str(row.iloc[7]) if pd.notna(row.iloc[7]) else ""
                    result['margin'] = str(row.iloc[8]) if pd.notna(row.iloc[8]) else ""
                    
                    # 人気、オッズ（列位置が異なる場合がある）
                    if len(row) > 13:
                        result['popularity'] = int(row.iloc[13]) if pd.notna(row.iloc[13]) else 0
                    if len(row) > 12:
                        odds_val = row.iloc[12]
                        result['odds'] = float(odds_val) if pd.notna(odds_val) else 0.0
                    
                    # 上がり3F
                    if len(row) > 11:
                        result['last_3f'] = str(row.iloc[11]) if pd.notna(row.iloc[11]) else ""
                    
                    # 通過順
                    if len(row) > 10:
                        result['corner_pass'] = str(row.iloc[10]) if pd.notna(row.iloc[10]) else ""
                    
                    # 調教師
                    if len(row) > 18:
                        trainer_text = str(row.iloc[18]) if pd.notna(row.iloc[18]) else ""
                        trainer_parts = trainer_text.split()
                        if len(trainer_parts) >= 2:
                            result['trainer_location'] = trainer_parts[0]
                            result['trainer_name'] = trainer_parts[1]
                        else:
                            result['trainer_location'] = ""
                            result['trainer_name'] = trainer_text
                    
                    # 馬体重
                    if len(row) > 14:
                        weight_text = str(row.iloc[14]) if pd.notna(row.iloc[14]) else ""
                        weight_match = re.search(r'(\d+)\(([+-]?\d+)\)', weight_text)
                        if weight_match:
                            result['horse_weight'] = int(weight_match.group(1))
                            result['weight_change'] = int(weight_match.group(2))
                        else:
                            result['horse_weight'] = 0
                            result['weight_change'] = 0
                    
                    results.append(result)
                    
                except Exception as e:
                    logger.warning(f"Error extracting horse result at index {idx}: {e}")
                    continue
            
        except Exception as e:
            logger.error(f"Error extracting race results: {e}")
        
        return results
    
    def _extract_payback_info(self, tables: List[pd.DataFrame]) -> Dict:
        """払い戻し情報を抽出"""
        payback = {}
        
        try:
            # 通常、table[1]とtable[2]に払い戻し情報がある
            if len(tables) > 1:
                # 単勝、複勝、枠連、馬連（左側）
                left_table = tables[1]
                for idx, row in left_table.iterrows():
                    try:
                        bet_type = str(row.iloc[0])
                        result = str(row.iloc[1]) if len(row) > 1 else ""
                        payout = str(row.iloc[2]) if len(row) > 2 else ""
                        popularity = str(row.iloc[3]) if len(row) > 3 else ""
                        
                        payback[bet_type] = {
                            'result': result,
                            'payout': payout,
                            'popularity': popularity
                        }
                    except Exception as e:
                        logger.warning(f"Error extracting payback row: {e}")
                        continue
            
            if len(tables) > 2:
                # ワイド、馬単、3連複、3連単（右側）
                right_table = tables[2]
                for idx, row in right_table.iterrows():
                    try:
                        bet_type = str(row.iloc[0])
                        result = str(row.iloc[1]) if len(row) > 1 else ""
                        payout = str(row.iloc[2]) if len(row) > 2 else ""
                        popularity = str(row.iloc[3]) if len(row) > 3 else ""
                        
                        payback[bet_type] = {
                            'result': result,
                            'payout': payout,
                            'popularity': popularity
                        }
                    except Exception as e:
                        logger.warning(f"Error extracting payback row: {e}")
                        continue
                        
        except Exception as e:
            logger.error(f"Error extracting payback info: {e}")
        
        return payback
    
    def _extract_corner_pass(self, soup) -> Dict:
        """コーナー通過順を抽出"""
        corner_pass = {}
        
        try:
            corner_table = soup.find('table', class_='Corner_Num')
            if corner_table:
                rows = corner_table.find_all('tr')
                for row in rows:
                    try:
                        th = row.find('th')
                        td = row.find('td')
                        if th and td:
                            corner_name = th.get_text(strip=True)
                            pass_order = td.get_text(strip=True)
                            corner_pass[corner_name] = pass_order
                    except Exception as e:
                        logger.warning(f"Error extracting corner pass row: {e}")
                        continue
        except Exception as e:
            logger.error(f"Error extracting corner pass: {e}")
        
        return corner_pass
    
    def _extract_lap_times(self, soup) -> Dict:
        """ラップタイムを抽出"""
        lap_times = {
            'cumulative': [],
            'intervals': []
        }
        
        try:
            lap_table = soup.find('table', class_='Race_HaronTime')
            if lap_table:
                rows = lap_table.find_all('tr', class_='HaronTime')
                
                if len(rows) >= 2:
                    # 1行目：累積タイム
                    cumulative_cells = rows[0].find_all('td')
                    lap_times['cumulative'] = [cell.get_text(strip=True) for cell in cumulative_cells]
                    
                    # 2行目：区間タイム
                    interval_cells = rows[1].find_all('td')
                    lap_times['intervals'] = [cell.get_text(strip=True) for cell in interval_cells]
                
                # ペース
                pace_elem = soup.find('div', class_='RapPace_Title')
                if pace_elem:
                    pace_span = pace_elem.find('span')
                    lap_times['pace'] = pace_span.get_text(strip=True) if pace_span else None
                
        except Exception as e:
            logger.error(f"Error extracting lap times: {e}")
        
        return lap_times
    
    def _save_json(self, race_id: str, race_data: Dict):
        """JSONファイルに保存"""
        try:
            output_dir = Path("data/race_details")
            output_dir.mkdir(parents=True, exist_ok=True)
            
            output_file = output_dir / f"race_{race_id}_details.json"
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(race_data, f, ensure_ascii=False, indent=2)
            
            logger.debug(f"JSON saved: {output_file}")
        except Exception as e:
            logger.warning(f"Error saving JSON: {e}")
    
    def _update_database(self, race_id: str, race_data: Dict) -> bool:
        """データベースを更新"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            race_info = race_data.get('race_info', {})
            
            # race_idでレコードを検索
            cursor.execute("SELECT id FROM races WHERE race_id = ?", (race_id,))
            result = cursor.fetchone()
            
            if not result:
                logger.warning(f"Race not found in database: race_id={race_id}")
                conn.close()
                return False
            
            # 更新
            cursor.execute("""
                UPDATE races
                SET 
                    venue = ?,
                    distance = ?,
                    track_type = ?,
                    track_condition = ?,
                    horse_count = ?
                WHERE race_id = ?
            """, (
                race_info.get('venue', '不明'),
                race_info.get('distance', 0),
                race_info.get('track_type', '不明'),
                race_info.get('track_condition'),
                race_info.get('horse_count', 0),
                race_id
            ))
            
            conn.commit()
            
            if cursor.rowcount > 0:
                logger.debug(f"Updated race_id={race_id}: {race_info.get('venue')}, {race_info.get('track_type')}, {race_info.get('distance')}m")
                conn.close()
                return True
            else:
                logger.warning(f"No rows updated for race_id={race_id}")
                conn.close()
                return False
                
        except Exception as e:
            logger.error(f"Database update error for race_id={race_id}: {e}")
            if conn:
                conn.close()
            return False


def scrape_race_detail(race_id: str, db_path: str = "data/keiba.db") -> bool:
    """
    単一のレース詳細を取得してJSONとDBに保存（バッチ処理用）
    
    Args:
        race_id: レースID
        db_path: データベースパス
        
    Returns:
        成功時True、失敗時False
    """
    scraper = RaceDetailScraperPandas(db_path)
    return scraper.scrape_and_update(race_id)
