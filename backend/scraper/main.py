"""
ãƒ¡ã‚¤ãƒ³ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚¹ã‚¯ãƒªãƒ—ãƒˆï¼ˆ--limit, --offsetå¯¾å¿œç‰ˆï¼‰
"""
import sys
import os

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’sys.pathã«è¿½åŠ ï¼ˆModuleNotFoundErrorå¯¾ç­–ï¼‰
project_root = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
if project_root not in sys.path:
    sys.path.insert(0, project_root)

from backend.scraper.predictor_list import PredictorListScraper
from backend.scraper.prediction import PredictionScraper
from backend.database import SessionLocal, init_db
from backend.models.database import Predictor, Prediction, Race
from loguru import logger
import argparse
from datetime import datetime


def save_predictors(predictors_data: list):
    """äºˆæƒ³å®¶æƒ…å ±ã‚’ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ä¿å­˜"""
    db = SessionLocal()
    try:
        saved_count = 0
        for predictor_data in predictors_data:
            # æ—¢å­˜ã®äºˆæƒ³å®¶ã‚’ãƒã‚§ãƒƒã‚¯
            existing = db.query(Predictor).filter(
                Predictor.netkeiba_id == predictor_data['netkeiba_id']
            ).first()
            
            if existing:
                # æ—¢å­˜ã®äºˆæƒ³å®¶ã‚’æ›´æ–°
                existing.name = predictor_data['name']
                existing.updated_at = datetime.utcnow()
                logger.debug(f"Updated predictor: {predictor_data['name']}")
            else:
                # æ–°è¦äºˆæƒ³å®¶ã‚’ä½œæˆ
                predictor = Predictor(
                    netkeiba_id=predictor_data['netkeiba_id'],
                    name=predictor_data['name']
                )
                db.add(predictor)
                saved_count += 1
                logger.debug(f"Created new predictor: {predictor_data['name']}")
        
        db.commit()
        logger.info(f"Saved {saved_count} new predictors to database")
        
    except Exception as e:
        logger.error(f"Error saving predictors: {e}")
        db.rollback()
    finally:
        db.close()


def save_predictions(predictor_id: int, predictions_data: list):
    """äºˆæƒ³æƒ…å ±ã‚’ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ä¿å­˜"""
    db = SessionLocal()
    try:
        saved_count = 0
        
        # äºˆæƒ³å®¶ã‚’å–å¾—
        predictor = db.query(Predictor).filter(
            Predictor.netkeiba_id == predictor_id
        ).first()
        
        if not predictor:
            logger.error(f"Predictor {predictor_id} not found in database")
            return
        
        for pred_data in predictions_data:
            # ãƒ¬ãƒ¼ã‚¹åãŒãªã„äºˆæƒ³ã¯ã‚¹ã‚­ãƒƒãƒ—
            if not pred_data.get('race_name'):
                logger.debug(f"Skipping prediction with no race_name: {pred_data.get('prediction_id')}")
                continue
            
            # æ—¢å­˜ã®äºˆæƒ³ã‚’ãƒã‚§ãƒƒã‚¯
            if pred_data.get('prediction_id'):
                existing = db.query(Prediction).filter(
                    Prediction.netkeiba_prediction_id == pred_data['prediction_id']
                ).first()
                
                if existing:
                    logger.debug(f"Prediction {pred_data['prediction_id']} already exists")
                    continue
            
            # ãƒ¬ãƒ¼ã‚¹ã‚’ä½œæˆã¾ãŸã¯å–å¾—
            race = None
            if pred_data.get('race_name'):
                # ç°¡æ˜“çš„ãªãƒ¬ãƒ¼ã‚¹ä½œæˆï¼ˆè©³ç´°ã¯å¾Œã§æ›´æ–°å¯èƒ½ï¼‰
                race = Race(
                    race_id=f"temp_{predictor_id}_{pred_data.get('prediction_id', 0)}",
                    race_name=pred_data['race_name'],
                    race_date=pred_data.get('race_date', datetime.utcnow()),
                    venue=pred_data.get('venue', 'ä¸æ˜'),
                    grade=pred_data.get('grade'),
                    distance=0,  # å¾Œã§æ›´æ–°
                    track_type='ä¸æ˜',  # å¾Œã§æ›´æ–°
                    is_grade_race=pred_data.get('grade') in ['G1', 'G2', 'G3']
                )
                db.add(race)
                db.flush()  # IDã‚’å–å¾—
            
            # äºˆæƒ³ã‚’ä½œæˆ
            prediction = Prediction(
                predictor_id=predictor.id,
                race_id=race.id if race else None,
                netkeiba_prediction_id=pred_data.get('prediction_id'),
                predicted_at=pred_data.get('race_date', datetime.utcnow()),
                is_hit=pred_data.get('is_hit'),
                payout=pred_data.get('payout')
            )
            
            db.add(prediction)
            saved_count += 1
        
        # äºˆæƒ³å®¶ã®ç·äºˆæƒ³æ•°ã‚’æ›´æ–°
        predictor.total_predictions = db.query(Prediction).filter(
            Prediction.predictor_id == predictor.id
        ).count()
        
        # é‡è³äºˆæƒ³æ•°ã‚’æ›´æ–°
        predictor.grade_race_predictions = db.query(Prediction).join(Race).filter(
            Prediction.predictor_id == predictor.id,
            Race.is_grade_race == True
        ).count()
        
        # ä¿¡é ¼åº¦ã‚’æ›´æ–°
        if predictor.grade_race_predictions >= 15:
            predictor.data_reliability = "high"
        elif predictor.grade_race_predictions >= 10:
            predictor.data_reliability = "medium"
        else:
            predictor.data_reliability = "low"
        
        db.commit()
        logger.info(f"Saved {saved_count} predictions for predictor {predictor_id}")
        
    except Exception as e:
        logger.error(f"Error saving predictions for predictor {predictor_id}: {e}")
        db.rollback()
    finally:
        db.close()


def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    # å¼•æ•°ãƒ‘ãƒ¼ã‚µãƒ¼ã‚’è¨­å®š
    parser = argparse.ArgumentParser(description='ç«¶é¦¬äºˆæƒ³å®¶ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°')
    parser.add_argument('--limit', type=int, default=None, help='å‡¦ç†ã™ã‚‹äºˆæƒ³å®¶ã®æ•°')
    parser.add_argument('--offset', type=int, default=0, help='é–‹å§‹ä½ç½®ï¼ˆã‚¹ã‚­ãƒƒãƒ—ã™ã‚‹äºˆæƒ³å®¶ã®æ•°ï¼‰')
    parser.add_argument('--test', action='store_true', help='ãƒ†ã‚¹ãƒˆãƒ¢ãƒ¼ãƒ‰ï¼ˆæœ€åˆã®5äººã®ã¿ï¼‰')
    
    args = parser.parse_args()
    
    logger.info("Starting scraping process...")
    logger.info(f"Arguments: limit={args.limit}, offset={args.offset}, test={args.test}")
    
    # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚’åˆæœŸåŒ–ï¼ˆåˆå›ã®ã¿ï¼‰
    try:
        init_db()
    except Exception as e:
        logger.warning(f"Database may already exist: {e}")
    
    # äºˆæƒ³å®¶ä¸€è¦§ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼
    predictor_scraper = PredictorListScraper()
    
    # ãƒ­ã‚°ã‚¤ãƒ³ï¼ˆæœ‰æ–™ä¼šå“¡ã®å ´åˆï¼‰
    predictor_scraper.login()
    
    # äºˆæƒ³å®¶ä¸€è¦§ã‚’å–å¾—
    logger.info("Fetching predictor list...")
    predictors = predictor_scraper.get_all_active_predictors()
    
    if not predictors:
        logger.error("No predictors found. Exiting.")
        return
    
    logger.info(f"Found {len(predictors)} predictors")
    
    # äºˆæƒ³å®¶ã‚’ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ä¿å­˜
    save_predictors(predictors)
    
    # å„äºˆæƒ³å®¶ã®äºˆæƒ³å±¥æ­´ã‚’å–å¾—
    prediction_scraper = PredictionScraper()
    prediction_scraper.login()
    
    # å‡¦ç†ç¯„å›²ã‚’æ±ºå®š
    if args.test:
        # ãƒ†ã‚¹ãƒˆãƒ¢ãƒ¼ãƒ‰ï¼šæœ€åˆã®5äºº
        start_idx = 0
        end_idx = min(5, len(predictors))
        logger.info("Running in TEST mode - processing first 5 predictors only")
    else:
        # é€šå¸¸ãƒ¢ãƒ¼ãƒ‰ï¼šoffset ã¨ limit ã‚’ä½¿ç”¨
        start_idx = args.offset
        if args.limit:
            end_idx = min(start_idx + args.limit, len(predictors))
        else:
            end_idx = len(predictors)
    
    target_predictors = predictors[start_idx:end_idx]
    total_count = len(target_predictors)
    
    logger.info(f"Processing predictors [{start_idx}:{end_idx}] ({total_count} predictors)")
    logger.info(f"Total predictors in list: {len(predictors)}")
    
    for i, predictor_data in enumerate(target_predictors, 1):
        predictor_id = predictor_data['netkeiba_id']
        predictor_name = predictor_data['name']
        actual_index = start_idx + i - 1
        
        logger.info(f"[{i}/{total_count}] [index {actual_index}] Processing: {predictor_name} (ID: {predictor_id})")
        
        # äºˆæƒ³å±¥æ­´ã‚’å–å¾—
        predictions = prediction_scraper.get_predictor_predictions(predictor_id, limit=50)
        
        if predictions:
            # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ä¿å­˜
            save_predictions(predictor_id, predictions)
        else:
            logger.warning(f"No predictions found for predictor {predictor_id}")
    
    logger.info("Scraping process completed!")
    logger.info(f"Processed {total_count} predictors [index {start_idx} to {end_idx-1}]")
    
    # æ¬¡ã®offsetã‚’æ˜ç¤ºçš„ã«è¡¨ç¤º
    if end_idx < len(predictors):
        logger.info(f"âœ… Next offset: {end_idx}")
        logger.info(f"âœ… Next command: python backend/scraper/main.py --limit 10 --offset {end_idx}")
        remaining = len(predictors) - end_idx
        logger.info(f"ğŸ“Š Remaining: {remaining} predictors")
    else:
        logger.info("ğŸ‰ All predictors in list have been processed!")
    
    # çµ±è¨ˆã‚’è¡¨ç¤º
    db = SessionLocal()
    try:
        total_predictors = db.query(Predictor).count()
        processed_predictors = db.query(Predictor).filter(Predictor.total_predictions > 0).count()
        total_predictions = db.query(Prediction).count()
        high_reliability = db.query(Predictor).filter(Predictor.data_reliability == "high").count()
        
        logger.info(f"\n=== Statistics ===")
        logger.info(f"Total predictors in DB: {total_predictors}")
        logger.info(f"Processed predictors: {processed_predictors}/{total_predictors}")
        logger.info(f"Total predictions: {total_predictions}")
        logger.info(f"High reliability predictors: {high_reliability}")
        
    finally:
        db.close()


if __name__ == "__main__":
    # ãƒ­ã‚°è¨­å®š
    logger.add("logs/scraper_{time}.log", rotation="1 day", retention="7 days")
    
    try:
        main()
    except KeyboardInterrupt:
        logger.info("Scraping interrupted by user")
    except Exception as e:
        logger.exception(f"Unexpected error: {e}")
    finally:
        # Chromeãƒ—ãƒ­ã‚»ã‚¹ã‚’å¼·åˆ¶çµ‚äº†
        import os
        import signal
        try:
            os.system("taskkill /F /IM chrome.exe /T 2>nul")
            os.system("taskkill /F /IM chromedriver.exe /T 2>nul")
        except:
            pass
        
        # ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã‚’æ˜ç¤ºçš„ã«çµ‚äº†
        import sys
        sys.exit(0)
