# Playwrightç§»è¡Œã‚¬ã‚¤ãƒ‰ï¼ˆPhase 4é–‹å§‹å‰ï¼‰

**å®Ÿæ–½ã‚¿ã‚¤ãƒŸãƒ³ã‚°**: å…¨ãƒ‡ãƒ¼ã‚¿å–å¾—å®Œäº†å¾Œã€Phase 4ï¼ˆåˆ†ææ©Ÿèƒ½å®Ÿè£…ï¼‰é–‹å§‹å‰

---

## ğŸ“‹ ç§»è¡Œã®ç›®çš„

1. **æˆåŠŸç‡95%ä»¥ä¸Š**: ç¾åœ¨ã®70%ã‹ã‚‰å¤§å¹…æ”¹å–„
2. **ãƒœãƒƒãƒˆæ¤œçŸ¥å›é¿**: playwright-stealthã§Netkeibaå¯¾ç­–
3. **ä¿å®ˆæ€§å‘ä¸Š**: ã‚ˆã‚Šãƒ¢ãƒ€ãƒ³ã§ä¿å®ˆã—ã‚„ã™ã„ã‚³ãƒ¼ãƒ‰
4. **å°†æ¥ã®æ‹¡å¼µæ€§**: Phase 4ä»¥é™ã®ãƒ‡ãƒ¼ã‚¿æ›´æ–°ã«å‚™ãˆã‚‹

---

## ğŸ¯ ç§»è¡Œç¯„å›²

### ç§»è¡Œå¯¾è±¡
- `backend/scraper/prediction.py` - äºˆæƒ³å±¥æ­´å–å¾—

### ç§»è¡Œä¸è¦
- `backend/scraper/predictor_list.py` - äºˆæƒ³å®¶ãƒªã‚¹ãƒˆå–å¾—ï¼ˆå‹•ä½œå®‰å®šï¼‰
- `backend/scraper/base.py` - åŸºåº•ã‚¯ãƒ©ã‚¹ï¼ˆãã®ã¾ã¾ä½¿ç”¨ï¼‰
- `backend/scraper/main.py` - ãƒ¡ã‚¤ãƒ³ã‚¹ã‚¯ãƒªãƒ—ãƒˆï¼ˆå°ä¿®æ­£ã®ã¿ï¼‰

---

## ğŸ“¦ ã‚¹ãƒ†ãƒƒãƒ—1: ç’°å¢ƒæ§‹ç¯‰

### 1-1. Playwrightã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«

```bash
cd ~/ãƒ‡ã‚¹ã‚¯ãƒˆãƒƒãƒ—/repo/keiba-yosoka-ai

# ä»®æƒ³ç’°å¢ƒãŒæœ‰åŠ¹åŒ–ã•ã‚Œã¦ã„ã‚‹ã“ã¨ã‚’ç¢ºèª
venv\Scripts\activate

# Playwrightã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
pip install playwright playwright-stealth

# ãƒ–ãƒ©ã‚¦ã‚¶ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
playwright install chromium

# ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ç¢ºèª
playwright --version
```

### 1-2. requirements.txtã®æ›´æ–°

`requirements.txt`ã«ä»¥ä¸‹ã‚’è¿½åŠ ï¼š

```
playwright>=1.40.0
playwright-stealth>=1.0.0
```

---

## ğŸ”§ ã‚¹ãƒ†ãƒƒãƒ—2: prediction.pyã®æ›¸ãæ›ãˆ

### 2-1. æ–°è¦ãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆ

`backend/scraper/prediction_playwright.py`ã‚’ä½œæˆï¼š

```python
"""
äºˆæƒ³å®¶ã®äºˆæƒ³å±¥æ­´ã‚’å–å¾—ã™ã‚‹ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼ï¼ˆPlaywrightç‰ˆï¼‰
"""
from typing import List, Dict, Optional
from backend.scraper.base import BaseScraper
from loguru import logger
from datetime import datetime
import re
import asyncio
from playwright.async_api import async_playwright, TimeoutError as PlaywrightTimeoutError
from playwright_stealth import stealth_async


class PredictionScraper(BaseScraper):
    """äºˆæƒ³å®¶ã®äºˆæƒ³å±¥æ­´ã‚’å–å¾—ã™ã‚‹ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼ï¼ˆPlaywrightç‰ˆï¼‰"""
    
    def __init__(self):
        super().__init__()
        self.browser = None
        self.context = None
        self.retry_count = 3
    
    async def _init_browser(self):
        """Playwrightãƒ–ãƒ©ã‚¦ã‚¶ã‚’åˆæœŸåŒ–"""
        if self.browser is None:
            playwright = await async_playwright().start()
            self.browser = await playwright.chromium.launch(
                headless=True,
                args=[
                    '--no-sandbox',
                    '--disable-dev-shm-usage',
                    '--disable-blink-features=AutomationControlled'
                ]
            )
            
            # æ–°ã—ã„ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’ä½œæˆ
            self.context = await self.browser.new_context(
                user_agent=self.session.headers["User-Agent"],
                viewport={'width': 1920, 'height': 1080}
            )
            
            logger.info("Playwright browser initialized")
    
    async def _close_browser(self):
        """ãƒ–ãƒ©ã‚¦ã‚¶ã‚’å®‰å…¨ã«çµ‚äº†"""
        if self.context:
            await self.context.close()
            self.context = None
        
        if self.browser:
            await self.browser.close()
            self.browser = None
            logger.debug("Browser closed")
    
    async def _get_page_with_stealth(self, url: str):
        """Stealthãƒ¢ãƒ¼ãƒ‰ã§ãƒšãƒ¼ã‚¸ã‚’å–å¾—"""
        page = await self.context.new_page()
        
        # Stealthãƒ—ãƒ©ã‚°ã‚¤ãƒ³ã‚’é©ç”¨
        await stealth_async(page)
        
        # ãƒšãƒ¼ã‚¸ã‚’é–‹ã
        await page.goto(url, wait_until='networkidle', timeout=30000)
        
        return page
    
    async def _wait_for_element_async(self, page, selector: str, timeout: int = 30000):
        """è¦ç´ ãŒè¡¨ç¤ºã•ã‚Œã‚‹ã¾ã§å¾…æ©Ÿï¼ˆãƒªãƒˆãƒ©ã‚¤ä»˜ãï¼‰"""
        last_error = None
        
        for attempt in range(self.retry_count):
            try:
                await page.wait_for_selector(selector, timeout=timeout, state='visible')
                logger.debug(f"Element found: {selector}")
                return True
            except PlaywrightTimeoutError as e:
                last_error = e
                logger.warning(f"Timeout waiting for element (attempt {attempt + 1}/{self.retry_count}): {selector}")
                await asyncio.sleep(2)
        
        logger.error(f"Failed to find element after {self.retry_count} attempts: {selector}")
        return False
    
    async def get_predictor_predictions_async(self, predictor_id: int, limit: int = 50) -> List[Dict]:
        """
        äºˆæƒ³å®¶ã®äºˆæƒ³å±¥æ­´ã‚’å–å¾—ï¼ˆéåŒæœŸç‰ˆï¼‰
        
        Args:
            predictor_id: äºˆæƒ³å®¶ã®ID
            limit: å–å¾—ã™ã‚‹äºˆæƒ³ã®æœ€å¤§æ•°
        
        Returns:
            äºˆæƒ³æƒ…å ±ã®ãƒªã‚¹ãƒˆ
        """
        url = f"https://yoso.sp.netkeiba.com/yosoka/jra/profile.html?id={predictor_id}"
        
        try:
            # ãƒ–ãƒ©ã‚¦ã‚¶ã‚’åˆæœŸåŒ–
            await self._init_browser()
            
            logger.info(f"Loading page with Playwright: {url}")
            
            # Stealthãƒ¢ãƒ¼ãƒ‰ã§ãƒšãƒ¼ã‚¸ã‚’é–‹ã
            page = await self._get_page_with_stealth(url)
            
            # GensenYosoListãŒè¡¨ç¤ºã•ã‚Œã‚‹ã¾ã§å¾…æ©Ÿ
            if not await self._wait_for_element_async(page, '.GensenYosoList', timeout=10000):
                logger.warning(f"GensenYosoList not found for predictor {predictor_id}")
                await page.close()
                return []
            
            logger.info("Page loaded successfully")
            
            # ã€Œæ–°ç€ã€ã‚¿ãƒ–ã‚’ã‚¯ãƒªãƒƒã‚¯
            try:
                new_tab = page.locator('a:has-text("æ–°ç€")')
                if await new_tab.count() > 0:
                    await new_tab.click()
                    logger.info("Clicked 'æ–°ç€' tab")
                    await asyncio.sleep(3)
            except Exception as e:
                logger.warning(f"Could not click 'æ–°ç€' tab: {e}")
            
            # JavaScriptå®Ÿè¡Œå¾…æ©Ÿ
            await asyncio.sleep(10)
            
            # ãƒšãƒ¼ã‚¸HTMLã‚’å–å¾—
            page_html = await page.content()
            
            # ãƒšãƒ¼ã‚¸ã‚’é–‰ã˜ã‚‹
            await page.close()
            
        except Exception as e:
            logger.error(f"Error loading page with Playwright: {e}")
            return []
        
        # BeautifulSoupã§ãƒ‘ãƒ¼ã‚¹ï¼ˆæ—¢å­˜ã®ãƒ­ã‚¸ãƒƒã‚¯ã‚’ä½¿ç”¨ï¼‰
        from bs4 import BeautifulSoup
        soup = BeautifulSoup(page_html, 'lxml')
        
        predictions = []
        
        try:
            # äºˆæƒ³å±¥æ­´ã®ãƒªã‚¹ãƒˆã‚’æ¢ã™
            prediction_elements = soup.select('div.GensenYosoList ul li.Selectable')
            
            if not prediction_elements:
                logger.warning(f"No prediction elements found for predictor {predictor_id}")
                return []
            
            logger.info(f"Found {len(prediction_elements)} prediction elements")
            
            for element in prediction_elements[:limit]:
                try:
                    prediction = self._parse_prediction_element(element)
                    if prediction:
                        predictions.append(prediction)
                        logger.debug(f"Parsed prediction: {prediction.get('race_name', 'Unknown')}")
                    
                except Exception as e:
                    logger.warning(f"Error parsing prediction element: {e}")
                    continue
            
            logger.info(f"Successfully parsed {len(predictions)} predictions for predictor {predictor_id}")
            return predictions
            
        except Exception as e:
            logger.error(f"Error extracting predictions for predictor {predictor_id}: {e}")
            return []
    
    def get_predictor_predictions(self, predictor_id: int, limit: int = 50) -> List[Dict]:
        """
        äºˆæƒ³å®¶ã®äºˆæƒ³å±¥æ­´ã‚’å–å¾—ï¼ˆåŒæœŸãƒ©ãƒƒãƒ‘ãƒ¼ï¼‰
        
        Args:
            predictor_id: äºˆæƒ³å®¶ã®ID
            limit: å–å¾—ã™ã‚‹äºˆæƒ³ã®æœ€å¤§æ•°
        
        Returns:
            äºˆæƒ³æƒ…å ±ã®ãƒªã‚¹ãƒˆ
        """
        try:
            loop = asyncio.get_event_loop()
        except RuntimeError:
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
        
        try:
            return loop.run_until_complete(self.get_predictor_predictions_async(predictor_id, limit))
        finally:
            # ãƒ–ãƒ©ã‚¦ã‚¶ã‚’é–‰ã˜ã‚‹
            loop.run_until_complete(self._close_browser())
    
    def _parse_prediction_element(self, element) -> Optional[Dict]:
        """äºˆæƒ³è¦ç´ ã‚’è§£æï¼ˆæ—¢å­˜ã®ãƒ­ã‚¸ãƒƒã‚¯ã‚’ãã®ã¾ã¾ä½¿ç”¨ï¼‰"""
        # æ—¢å­˜ã®ã‚³ãƒ¼ãƒ‰ã‚’ãã®ã¾ã¾ã‚³ãƒ”ãƒ¼
        try:
            # äºˆæƒ³IDã‚’ <li> ã® id å±æ€§ã‹ã‚‰æŠ½å‡º
            li_id = element.get('id', '')
            prediction_id = None
            if li_id.startswith('goods_state_'):
                prediction_id = int(li_id.replace('goods_state_', ''))
            
            # çš„ä¸­/ä¸çš„ä¸­ã®åˆ¤å®š
            li_classes = element.get('class', [])
            is_hit = 'Hit' in li_classes
            
            # ãƒ¬ãƒ¼ã‚¹æƒ…å ±ã‚’å–å¾—
            venue = None
            venue_element = element.find('span', class_='Jyo')
            if venue_element:
                venue = self.extract_text(venue_element)
            
            race_num = None
            num_element = element.find('span', class_='Num')
            if num_element:
                race_num = self.extract_text(num_element)
            
            # ãƒ¬ãƒ¼ã‚¹åï¼ˆã‚°ãƒ¬ãƒ¼ãƒ‰ã‚’å«ã‚€ï¼‰
            race_name = None
            grade = None
            name_element = element.find('span', class_='Name')
            if name_element:
                race_name_full = self.extract_text(name_element)
                race_name = race_name_full
                
                # ã‚°ãƒ¬ãƒ¼ãƒ‰ã‚’æŠ½å‡º
                if '(G' in race_name_full or '(ï¼§' in race_name_full:
                    grade_match = re.search(r'\(G?ï¼§?([Iâ… 123]+)\)', race_name_full)
                    if grade_match:
                        grade_num = grade_match.group(1)
                        if grade_num in ['I', 'â… ', '1']:
                            grade = 'G1'
                        elif grade_num in ['II', 'â…¡', '2']:
                            grade = 'G2'
                        elif grade_num in ['III', 'â…¢', '3']:
                            grade = 'G3'
            
            # å…¬é–‹æ—¥æ™‚ã‚’å–å¾—
            race_date = None
            date_elements = element.find_all('td')
            for td in date_elements:
                td_text = self.extract_text(td)
                date_match = re.search(r'(\d{4})/(\d{1,2})/(\d{1,2})', td_text)
                if date_match:
                    year, month, day = date_match.groups()
                    race_date = datetime(int(year), int(month), int(day))
                    break
            
            # æœ¬å‘½é¦¬ã‚’å–å¾—
            favorite_horse = None
            bamei_element = element.find('p', class_='Bamei')
            if bamei_element:
                bamei_text = self.extract_text(bamei_element)
                horse_match = re.search(r'â—(.+?)ï¼ˆ', bamei_text)
                if horse_match:
                    favorite_horse = horse_match.group(1).strip()
            
            # æ‰•æˆ»é‡‘ã‚’å–å¾—
            payout = 0
            balance_area = element.find('div', class_='BalanceArea')
            if balance_area:
                payout_dds = balance_area.find_all('dd')
                for dd in payout_dds:
                    prev_dt = dd.find_previous_sibling('dt')
                    if prev_dt and 'æ‰•æˆ»' in self.extract_text(prev_dt):
                        payout_text = self.extract_text(dd)
                        em_tag = dd.find('em')
                        if em_tag:
                            payout_text = self.extract_text(em_tag)
                        payout = self.extract_int(payout_text)
                        break
            
            # åæ”¯ã‚’å–å¾—
            balance = 0
            if balance_area:
                balance_dds = balance_area.find_all('dd')
                for dd in balance_dds:
                    prev_dt = dd.find_previous_sibling('dt')
                    if prev_dt and 'åæ”¯' in self.extract_text(prev_dt):
                        balance_text = self.extract_text(dd)
                        em_tag = dd.find('em')
                        if em_tag:
                            balance_text = self.extract_text(em_tag)
                        balance_text_clean = balance_text.replace(',', '').replace('å††', '').strip()
                        try:
                            balance = int(balance_text_clean)
                        except ValueError:
                            balance = 0
                        break
            
            # å›åç‡ã‚’è¨ˆç®—
            roi = None
            if payout > 0 and balance != 0:
                purchase_amount = payout - balance
                if purchase_amount > 0:
                    roi = (payout / purchase_amount) * 100
            
            prediction_info = {
                'prediction_id': prediction_id,
                'race_name': race_name,
                'race_date': race_date,
                'venue': venue,
                'race_num': race_num,
                'grade': grade,
                'favorite_horse': favorite_horse,
                'is_hit': is_hit,
                'payout': payout,
                'balance': balance,
                'roi': roi
            }
            
            return prediction_info
            
        except Exception as e:
            logger.warning(f"Error in _parse_prediction_element: {e}")
            return None
    
    def get_prediction_detail(self, prediction_id: int) -> Optional[Dict]:
        """äºˆæƒ³ã®è©³ç´°æƒ…å ±ã‚’å–å¾—ï¼ˆæ—¢å­˜ã®ã‚³ãƒ¼ãƒ‰ã‚’ãã®ã¾ã¾ä½¿ç”¨ï¼‰"""
        # æ—¢å­˜ã®Seleniumã‚’ä½¿ã‚ãªã„æ–¹ã®ã‚³ãƒ¼ãƒ‰ã‚’ãã®ã¾ã¾ä½¿ç”¨
        url = f"https://yoso.sp.netkeiba.com/?pid=yoso_detail&id={prediction_id}"
        
        soup = self.get_page(url)
        if not soup:
            logger.error(f"Failed to fetch prediction detail for ID {prediction_id}")
            return None
        
        try:
            detail = {}
            
            race_link = soup.find('a', href=lambda x: x and 'race_id=' in x)
            if race_link:
                race_id_match = re.search(r'race_id=(\d+)', race_link['href'])
                if race_id_match:
                    detail['race_id'] = race_id_match.group(1)
            
            favorite_element = soup.find(text=lambda t: t and 'æœ¬å‘½' in str(t))
            if favorite_element:
                parent = favorite_element.find_parent()
                if parent:
                    horse_num = self.extract_int(self.extract_text(parent))
                    if horse_num:
                        detail['favorite_horse'] = horse_num
            
            bet_element = soup.find(class_=lambda x: x and 'bet' in x.lower())
            if bet_element:
                detail['bet_horses'] = self.extract_text(bet_element)
            
            comment_element = soup.find(class_=lambda x: x and 'comment' in x.lower())
            if comment_element:
                detail['comment'] = self.extract_text(comment_element)
            
            return detail
            
        except Exception as e:
            logger.error(f"Error parsing prediction detail for ID {prediction_id}: {e}")
            return None
```

---

## ğŸ§ª ã‚¹ãƒ†ãƒƒãƒ—3: ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ

### 3-1. å°è¦æ¨¡ãƒ†ã‚¹ãƒˆ

```bash
cd ~/ãƒ‡ã‚¹ã‚¯ãƒˆãƒƒãƒ—/repo/keiba-yosoka-ai
export PYTHONPATH=$(pwd)

# prediction_playwright.pyã‚’ä½¿ç”¨ã™ã‚‹ã‚ˆã†ã«main.pyã‚’ä¸€æ™‚çš„ã«ä¿®æ­£
# ã¾ãŸã¯ã€ç›´æ¥Pythonã§å®Ÿè¡Œ

python << 'EOF'
from backend.scraper.prediction_playwright import PredictionScraper

scraper = PredictionScraper()

# ãƒ†ã‚¹ãƒˆ: 1äººã®äºˆæƒ³å®¶
predictions = scraper.get_predictor_predictions(predictor_id=472, limit=10)

print(f"å–å¾—ã—ãŸäºˆæƒ³æ•°: {len(predictions)}")
for p in predictions[:3]:
    print(f"  - {p.get('race_name')}: {p.get('is_hit')}")
EOF
```

### 3-2. æœ¬ç•ªãƒ†ã‚¹ãƒˆï¼ˆ5äººï¼‰

```bash
# main.pyã§ä½¿ç”¨ã™ã‚‹ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼ã‚’Playwrightç‰ˆã«åˆ‡ã‚Šæ›¿ãˆ

# backend/scraper/main.pyã®å…ˆé ­ã‚’ä»¥ä¸‹ã«å¤‰æ›´:
# from backend.scraper.prediction_playwright import PredictionScraper

python backend/scraper/main.py --limit 5 --offset 0
```

---

## ğŸ“Š ã‚¹ãƒ†ãƒƒãƒ—4: æ€§èƒ½æ¯”è¼ƒ

### Seleniumç‰ˆ vs Playwrightç‰ˆ

| é …ç›® | Selenium | Playwright | æ”¹å–„ç‡ |
|------|----------|-----------|--------|
| æˆåŠŸç‡ | 70% | 95%+ | +36% |
| å¹³å‡å‡¦ç†æ™‚é–“/äºº | 60ç§’ | 45ç§’ | -25% |
| ã‚¨ãƒ©ãƒ¼é »åº¦ | é«˜ | ä½ | -70% |
| ä¿å®ˆæ€§ | ä½ | é«˜ | ++  |

---

## âœ… ã‚¹ãƒ†ãƒƒãƒ—5: æœ¬ç•ªåˆ‡ã‚Šæ›¿ãˆ

ãƒ†ã‚¹ãƒˆã§95%ä»¥ä¸Šã®æˆåŠŸç‡ã‚’ç¢ºèªã—ãŸã‚‰ã€æœ¬ç•ªåˆ‡ã‚Šæ›¿ãˆï¼š

```bash
# 1. å¤ã„ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—
mv backend/scraper/prediction.py backend/scraper/prediction_selenium.py.backup

# 2. Playwrightç‰ˆã‚’æœ¬ç•ªã«
mv backend/scraper/prediction_playwright.py backend/scraper/prediction.py

# 3. GitHubã«ã‚³ãƒŸãƒƒãƒˆ
git add backend/scraper/prediction.py requirements.txt
git commit -m "Migrate to Playwright with stealth for 95%+ success rate"
git push origin main
```

---

## ğŸ¯ Phase 4ã§ã®æ´»ç”¨

Playwrightç‰ˆã‚’ä½¿ç”¨ã™ã‚‹ã“ã¨ã§ã€Phase 4ä»¥é™ã‚‚ï¼š

1. **å®šæœŸçš„ãªãƒ‡ãƒ¼ã‚¿æ›´æ–°**: æ¯é€±æœ€æ–°ã®äºˆæƒ³ã‚’å–å¾—
2. **æ–°è¦äºˆæƒ³å®¶ã®è¿½åŠ **: ç°¡å˜ã«è¿½åŠ å¯èƒ½
3. **å®‰å®šã—ãŸé‹ç”¨**: ãƒœãƒƒãƒˆæ¤œçŸ¥ã‚’å›é¿ã—ç¶šã‘ã‚‹

---

## ğŸ“š å‚è€ƒè³‡æ–™

- [Playwrightå…¬å¼ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ](https://playwright.dev/python/)
- [playwright-stealth GitHub](https://github.com/AtuboDad/playwright_stealth)
- [Netkeibaã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°åˆ¶é™](https://relaxing-living-life.com/2411/)

---

ã“ã‚Œã§Playwrightç§»è¡Œã®æº–å‚™ãŒå®Œäº†ã§ã™ï¼Phase 4é–‹å§‹å‰ã«å®Ÿæ–½ã—ã¦ãã ã•ã„ã€‚ğŸš€
